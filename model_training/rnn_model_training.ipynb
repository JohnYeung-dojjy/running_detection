{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LSTM model to predict if the detected pose is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model_classes.RNN.v1 import RNNModel\n",
    "from action_classifier import OUTPUT_LABELS\n",
    "\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent \"RuntimeError: CUDA error: device-side assert triggered\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_OUTPUT_LABELS = [\n",
    "    \"nose_x\", \"nose_y\",\n",
    "    \"left_eye_x\", \"left_eye_y\",\n",
    "    \"right_eye_x\", \"right_eye_y\",\n",
    "    \"left_ear_x\", \"left_ear_y\",\n",
    "    \"right_ear_x\", \"right_ear_y\",\n",
    "    \"left_shoulder_x\", \"left_shoulder_y\",\n",
    "    \"right_shoulder_x\", \"right_shoulder_y\",\n",
    "    \"left_elbow_x\", \"left_elbow_y\",\n",
    "    \"right_elbow_x\", \"right_elbow_y\",\n",
    "    \"left_wrist_x\", \"left_wrist_y\",\n",
    "    \"right_wrist_x\", \"right_wrist_y\",\n",
    "    \"left_hip_x\", \"left_hip_y\",\n",
    "    \"right_hip_x\", \"right_hip_y\",\n",
    "    \"left_knee_x\", \"left_knee_y\",\n",
    "    \"right_knee_x\", \"right_knee_y\",\n",
    "    \"left_ankle_x\", \"left_ankle_y\",\n",
    "    \"right_ankle_x\", \"right_ankle_y\",\n",
    "]\n",
    "\n",
    "MODEL_INPUT_LABELS = [\n",
    "    \"right_shoulder_x\", \"right_shoulder_y\",\n",
    "    \"left_elbow_x\", \"left_elbow_y\",\n",
    "    \"right_elbow_x\", \"right_elbow_y\",\n",
    "    \"left_wrist_x\", \"left_wrist_y\",\n",
    "    \"right_wrist_x\", \"right_wrist_y\",\n",
    "    \"left_hip_x\", \"left_hip_y\",\n",
    "    \"right_hip_x\", \"right_hip_y\",\n",
    "    \"left_knee_x\", \"left_knee_y\",\n",
    "    \"right_knee_x\", \"right_knee_y\",\n",
    "    \"left_ankle_x\", \"left_ankle_y\",\n",
    "    \"right_ankle_x\", \"right_ankle_y\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_MODEL = \"ultralytics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {label: i for i, label in enumerate(OUTPUT_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset_action(filename: str|Path):\n",
    "    \"\"\"Get the action label from the filename.\"\"\"\n",
    "    filename = str(filename)\n",
    "    if \"walking\" in filename:\n",
    "        return \"walking\"\n",
    "    elif \"running\" in filename:\n",
    "        return \"running\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown action label in filename: {filename}\")\n",
    "\n",
    "def _process_dataset(dataset: pd.DataFrame, action: Literal[\"walking\", \"running\"]) -> pd.DataFrame:\n",
    "    \"\"\"Process the loaded dataset, assigning action labels to each frame. \"\"\"\n",
    "    # assign label\n",
    "    dataset = dataset.assign(action=LABELS[action])\n",
    "    # If the frame has no detection, assign as no_detection, deprecated due to the overwhelming of no_detection in the dataset\n",
    "    # dataset.loc[(dataset[POSE_OUTPUT_LABELS] == 0).any(axis=\"columns\"), \"action\"] = LABELS[\"no_detection\"]\n",
    "\n",
    "    # drop all no_detection frames\n",
    "    dataset = dataset.loc[(dataset[POSE_OUTPUT_LABELS]!=0).any(axis=\"columns\")]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_KTH_datasets(pose_model: Literal[\"mediapipe\", \"ultralytics\"], status: Literal[\"raw\", \"normalized\"]) -> list[pd.DataFrame]:\n",
    "    \"\"\"KTH dataset is recorded in 25 FPS, each frame contains the detected pose keypoint coordinates, all 0 if no person is detected.\"\"\"\n",
    "    dataset_dir = Path.cwd().parent / \"TrainingData\" / pose_model / status # this notebook is within model_training/, need to go back one level\n",
    "    dataset_list = []\n",
    "    for parquet_filename in tqdm(dataset_dir.iterdir(), desc=f\"Loading {pose_model} datasets\"):\n",
    "        if not parquet_filename.suffix == \".parquet\":\n",
    "            continue\n",
    "        action = _get_dataset_action(parquet_filename)\n",
    "        if not any(action in label for label in OUTPUT_LABELS): # ignore parquet files that are not used in training\n",
    "            continue\n",
    "        dataset = pd.read_parquet(parquet_filename)\n",
    "        dataset = _process_dataset(dataset, _get_dataset_action(parquet_filename))\n",
    "        dataset_list.append(dataset)\n",
    "\n",
    "    return dataset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = load_KTH_datasets(POSE_MODEL, \"normalized\")\n",
    "random.shuffle(dataset_list) # shuffle the dataset so videos dataframes of different actions are not next to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(\"action\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no shuffle for time series data\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[MODEL_INPUT_LABELS], dataset[\"action\"], test_size=0.2, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseActionDataset(Dataset):\n",
    "    \"\"\"We maintain a sliding window of certain length as model input shape\"\"\"\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor, window_length: int):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.window_length = window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.window_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx:idx+self.window_length], self.y[idx+self.window_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 50 # classifying every 50 frames window (2s)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 2**6\n",
    "n_epochs = 30\n",
    "lr = 1e-2\n",
    "\n",
    "input_dim = len(MODEL_INPUT_LABELS)\n",
    "hidden_dim = 32\n",
    "layer_dim = 1\n",
    "output_dim = len(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train.values).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train.values).to(device)\n",
    "y_train_tensor = torch.nn.functional.one_hot(y_train_tensor, num_classes=len(LABELS)).float()\n",
    "\n",
    "X_val_tensor = torch.from_numpy(X_val.values).float().to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val.values).to(device)\n",
    "y_val_tensor = torch.nn.functional.one_hot(y_val_tensor, num_classes=len(LABELS)).float()\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test.values).float().to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test.values).to(device)\n",
    "y_test_tensor = torch.nn.functional.one_hot(y_test_tensor, num_classes=len(LABELS)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseActionDataset(X_train_tensor, y_train_tensor, WINDOW_LENGTH)\n",
    "val_dataset = PoseActionDataset(X_val_tensor, y_val_tensor, WINDOW_LENGTH)\n",
    "test_dataset = PoseActionDataset(X_test_tensor, y_test_tensor, WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no shuffle for time series data\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(input_dim=input_dim, hidden_dim=hidden_dim, layer_dim=layer_dim, output_dim=output_dim).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i, (pose_seq, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the data were already put to corresponding device\n",
    "        outputs = model(pose_seq)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    scheduler.step()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (pose_seq, labels) in enumerate(val_dataloader):\n",
    "            outputs = model(pose_seq)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    training_losses.append(epoch_loss / len(train_dataloader))\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {epoch_loss / len(train_dataloader):.8f}, Validation Loss: {val_loss / len(val_dataloader):.8f}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_losses, label=\"Training Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "testing_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (pose_seq, labels) in enumerate(test_dataloader):\n",
    "        # the data were already put to corresponding device\n",
    "        outputs = model(pose_seq)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        testing_loss += loss.item()\n",
    "\n",
    "print(\"Testing Loss: \", testing_loss / len(test_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = Path.cwd().parent / \"models\" / \"RNN\" / f\"{input_dim}-{hidden_dim}-{layer_dim}-{output_dim}_WIN-{WINDOW_LENGTH}_EPOCH-{n_epochs}_LR-{lr}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "running_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
